---
layout: post
title: "Mechine learning原理公式整理"
keywords: ["ML","mechine learning","SVM","","Dtree","classification"]
description: "svm"
category: "Datascience"
tags: ["ML","Datascience"]
---

## 线性回归

一元方程可表示为:
  
$$ 
h_\theta(x)=\theta_0 + \theta_1 x
$$ 

多元方程可表示为:

$$ 
h_\theta(x)=\theta_0 + \theta_1 x_1+...+\theta_n x_n
$$ 

可以统一写成如下格式:

$$
h_\theta(x) =\sum_{i=0}^n (\theta_i x_i)
$$

其中$$x_0$$为0

梯度下降法

损失函数（loss/error fuction）为

$$
J(\theta) = \frac 1 2 \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2
$$

梯度(batch)计算

$$
\theta_j := \theta_j - \alpha\frac\partial{\partial\theta_j}J(\theta)
$$

又

$$
 \frac\partial{\partial\theta_j}(h_\theta(x)-y)^2
$$

$$
＝2  (h_\theta(x)-y) \frac\partial{\partial\theta_j}(h_\theta(x)-y)
$$

$$
＝2 (h_\theta(x)-y) \frac\partial{\partial\theta_j}(\sum_{i=0}^n (\theta_i x_i)-y)
$$
 
$$
＝2 (h_\theta(x)-y) x_j
$$

其中
$$
\sum_{i=0}^n (\theta_i x_i)
$$
由于是对
$$
\theta_j
$$
求导数，最后仅剩下
$$
x_j
$$

于是

$$
\frac\partial{\partial\theta_j}J(\theta)＝\frac\partial{\partial\theta_j} \frac 1 2 \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2
$$

$$
＝\sum_{i=1}^m \frac 1 2 2 (y^{(i)}-h_\theta(x^{(i)})) x_j^{(i)}
$$

$$
＝\sum_{i=1}^m  (y^{(i)}-h_\theta(x^{(i)})) x_j^{(i)}
$$

最终，梯度(batch)计算公式为

$$
\theta_j := \theta_j - \alpha \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)}) x_j^{(i)}
$$

$$
= \theta_j + \alpha \sum_{i=1}^m (y^{(i)}-h_\theta(x^{(i)})) x_j^{(i)}
$$

随机梯度下降法为
repeat{
      for i=1 to m {
$$
\theta_j := \theta_j + \alpha (y^{(i)}-h_\theta(x^{(i)})) x_j^{(i)}
$$      
         
         }
      
      }

$$

$$


### 梯度下降与回归方法

>
1. 从中心极限定理到正太分布到极大释然函数到平方和最小函数能求解出最佳theta
2. 批量梯度：每次迭代计算theta使用所有样本
3. 随机梯度：没读取一条样本就迭代对theta进行更新
4. 改进后的随机梯度下降



### 逻辑回归 softmax

>
1. Gradient Descent (SGD BGD)
2. 牛顿法
3. 拟牛顿法
4. BGFS
5. L-BGFS

#### 求解过程 注意误差函数：E（w）=∑[h（x-y]^2/2   梯度求解   wi=wi-η*∂E/∂wi  ∂E/∂wi=∑（h(x)-y*(xi)

### 决策树

>
1. ID3,C4.5  信息增益(最大信息增益)/信息增益率 
2. CART  基尼系数Gini 最好的划分就是使得GINI_Gain最小的划分。（回归树：最小平方残差、最小绝对残差等）



### 相关课程资源

>
1. cs231n
2. CS224d
3. cs229
